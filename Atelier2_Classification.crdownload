#!/usr/bin/env python
# coding: utf-8

# # Atelier 2 «Classification»

# # Partie 1 (Data Visualisation et Feature Selection et Normalisation):

# 1. En utilisant pandas essayer d’explorer les données du Data set.

# In[1]:


import pandas as pd


# In[2]:


# avoid warnings 
import warnings 
warnings.filterwarnings('ignore')
get_ipython().run_line_magic('matplotlib', 'inline')


# In[3]:


columns=["Preg","Glucose","Pres","Skin","Insulin","BMI","Pedi","Age","Outcome"]
df=pd.read_csv("pima-indians-diabetes.csv", names=columns)
df.head()


# 2. Afficher le résumer statistique du Data Sets avec une interprétation des résultats obtenues.

# In[4]:


df.describe()


# Interprétation :
# - count : le nombre d’enregistrement dans la Data sets.
# - mean : la moyenne de chaque colonne.
# - std : l’écart-type de chaque colonne.
# - min : la valeur minimale de chaque colonne.
# - 25% et 75% : les interquartiles de chaque colonne.
# - max : la valeur maximale de chaque colonne.

# 3. Afficher les nuages des points du data set selon les propriétés « Features » en utilisant matplotlib et
# pandas « scatter_matrix ».

# In[5]:


import matplotlib.pyplot as plt
from pandas.plotting import scatter_matrix


# In[6]:


scatter_matrix(df)
plt.show()


# 4. Appliquer les 4 méthodes de Features selection « Univariate Selection, PCA, Recursive Feature
# Elimination et Feature Importance ».

# # Univariate Selection

# In[7]:


from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
import numpy as np


# In[8]:


array = df.values
X = array[:,0:8]
Y = array[:,8]


# In[9]:


# feature extraction
test = SelectKBest(score_func=chi2, k=2)
fit = test.fit(X, Y)


# In[10]:


# summarize scores
np.set_printoptions(precision=3)
col = ["Preg","Glucose","Pres","Skin","Insulin","BMI","Pedi","Age","Outcome"]
print(columns)
print(fit.scores_)

features = fit.transform(X)

# summarize selected features
print(features[:,:])


# # Principal Component Analysis,PCA

# In[11]:


from sklearn.decomposition import PCA


# In[12]:


pca = PCA(n_components=3)
fit = pca.fit(X)

# summarize components
print("Explained Variance: %s" % fit.explained_variance_ratio_)
print(fit.components_)


# # Recursive Feature Elimination, RFE

# In[13]:


from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression


# In[14]:


model = LogisticRegression()
rfe = RFE(model, n_features_to_select=3)
fit = rfe.fit(X, Y)

print("Num Features: %d" % fit.n_features_)
print("Selected Features: %s" % fit.support_)
print("Feature Ranking: %s" % fit.ranking_)


# # Feature Importance, FI

# In[15]:


from sklearn.ensemble import ExtraTreesClassifier


# In[16]:


model = ExtraTreesClassifier()
model.fit(X, Y)
print(model.feature_importances_)


# 5. Normaliser les données des attributs qui nécessitent une normalisation.

# In[17]:


from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
print(scaler.fit(df))


# In[18]:


MinMaxScaler(copy=True, feature_range=(0, 1))
print(scaler.data_max_)


# In[19]:


print(scaler.transform(df))


# In[20]:


# RobustScaler
from sklearn.preprocessing import RobustScaler

transformer = RobustScaler().fit(X)
print(transformer)
RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,
with_scaling=True)
print(transformer.transform(X))


# In[21]:


#StandardScaler
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
print(scaler.fit(df))
print(scaler.mean_)
print(scaler.transform(df))


# # Partie 2 (Classification choix de algorithme adéquat ):  

# 1. En utilisant l’API sklearn entraîner les modèles en utilisant ces algorithmes « KNN, Decision Tree, ANN,
# Naive Bayes, SVM selon les kernels suivants : Linear, polynomial et guassain».

# # KNN

# In[22]:


from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)


# In[23]:


from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=10)
knn.fit(X_train, Y_train)


# In[26]:


#how to choice the K (I tried many values and I found that 14 is the best -> acc=79,22%)
acc = []
from sklearn import metrics
for i in range(1,30):
    neigh = KNeighborsClassifier(n_neighbors = i).fit(X_train,Y_train)
    yhat = neigh.predict(X_test) 
    acc.append(metrics.accuracy_score(Y_test, yhat))

plt.figure(figsize=(10,6))
plt.plot(range(1,30),acc,color = 'red',linestyle='dashed',marker='o',markerfacecolor='blue', markersize=10)
plt.title('accuracy vs. K Value')
plt.xlabel('K')
plt.ylabel('Accuracy')  


# # Decision Tree

# In[27]:


from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(random_state=0)
clf.fit(X_train, Y_train)

Y_pred = clf.predict(X_test)


# # ANN

# In[28]:


from sklearn.neural_network import MLPClassifier
ann = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000, alpha=1e-4,solver='adam', random_state=0, learning_rate_init=0.001)

ann.fit(X_train, Y_train)


# # Naive Bayes

# In[30]:


from sklearn.naive_bayes import GaussianNB

nb = GaussianNB()
nb.fit(X_train, Y_train)


# # Linear SVM

# In[31]:


from sklearn.svm import LinearSVC

lsvm = LinearSVC()
lsvm.fit(X_train, Y_train)


# # polynomial SVM  

# In[32]:


from sklearn.svm import SVC

psvm = SVC(kernel='poly', degree=3)
psvm.fit(X_train, Y_train)


# # gaussian SVM 

# In[33]:


gsvm = SVC(kernel='rbf')
gsvm.fit(X_train, Y_train)


# 2. Sauvegarder les 5 modèles

# In[34]:


from joblib import dump, load

# Sauvegarde les modèles
dump(knn, 'knn.joblib')
dump(clf, 'clf.joblib')
dump(ann, 'ann.joblib')
dump(nb, 'nb.joblib')
dump(lsvm, 'lsvm.joblib')
dump(psvm, 'psvm.joblib')
dump(gsvm, 'gsvm.joblib')


# 3. Évaluer les modèles en utilisant ces métriques:
# 
# Classification Accuracy.
# Logarithmic Loss.
# Area Under ROC Curve.
# Confusion Matrix.
# Classification Report.

# In[35]:


from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, log_loss, roc_auc_score


# In[40]:


print("KNN :")
print("Accuracy :",metrics.accuracy_score(Y_test, Y_pred)*100)
print("confusion_matrix : \n",  confusion_matrix(Y_test, Y_pred))
print("classification_report : \n", classification_report(Y_test, Y_pred))
print("ROC AUC Score:", roc_auc_score(Y_test, Y_pred))
print("Logarithmic Loss:", log_loss(Y_test, Y_pred))

print("--------------------------------------------------------------------")
print("DT :")
print("Accuracy :",metrics.accuracy_score(Y_test, Y_pred)*100)
print("confusion_matrix : \n",  confusion_matrix(Y_test, Y_pred))
print("classification_report : \n", classification_report(Y_test, Y_pred))
print("ROC AUC Score:", roc_auc_score(Y_test, Y_pred))
print("Logarithmic Loss:", log_loss(Y_test, Y_pred))

print("--------------------------------------------------------------------")

print("ANN :")
print("Accuracy :",metrics.accuracy_score(Y_test, Y_pred)*100)
print("confusion_matrix : \n",  confusion_matrix(Y_test, Y_pred))
print("classification_report : \n", classification_report(Y_test, Y_pred))
print("ROC AUC Score:", roc_auc_score(Y_test, Y_pred))
print("Logarithmic Loss:", log_loss(Y_test, Y_pred))

print("--------------------------------------------------------------------")
print("SVM Lieaire : ")
print("Accuracy :",metrics.accuracy_score(Y_test, Y_pred)*100)
print("confusion_matrix : \n",  confusion_matrix(Y_test, Y_pred))
print("classification_report : \n", classification_report(Y_test, Y_pred))
print("ROC AUC Score:", roc_auc_score(Y_test, Y_pred))
print("Logarithmic Loss:", log_loss(Y_test, Y_pred))

print("--------------------------------------------------------------------")
print("SVM polynomial :")
print("Accuracy : \n",metrics.accuracy_score(Y_test, Y_pred)*100)
print("confusion_matrix : \n",  confusion_matrix(Y_test, Y_pred))
print("classification_report : \n", classification_report(Y_test, Y_pred))
print("ROC AUC Score:", roc_auc_score(Y_test, Y_pred))
print("Logarithmic Loss:", log_loss(Y_test, Y_pred))

print("-------------------------------------------------------------------")
print("SVM gaussian :")
print("Accuracy :",metrics.accuracy_score(Y_test, Y_pred)*100)
print("confusion_matrix : \n",  confusion_matrix(Y_test, Y_pred))
print("classification_report : \n", classification_report(Y_test, Y_pred))
print("ROC AUC Score:", roc_auc_score(Y_test, Y_pred))
print("Logarithmic Loss:", log_loss(Y_test, Y_pred))


# 4. Comparer la performance des 8 algorithmes en utilisant la technique Spot-checking.

# In[ ]:





# 5. Charger les 5 modèles puis Prédire les données du data set de test, en utilisant les 8 modèles.

# In[43]:


# Chargement des modèles
knn = load('knn.joblib')
clf = load('clf.joblib')
ann = load('ann.joblib')
nb = load('nb.joblib')
lsvm = load('lsvm.joblib')
psvm = load('psvm.joblib')
gsvm = load('gsvm.joblib')


# In[55]:


print("_________________________KNN Prediction_______________________________")
print(knn.predict(X_test))
print("_________________________DT Prediction_______________________________")
print(clf.predict(X_test))
print("_________________________ANN Prediction_______________________________")
print(ann.predict(X_test))
print("_________________________NB Prediction_______________________________")
print(nb.predict(X_test))
print("_________________________LSVM Prediction_______________________________")
print(lsvm.predict(X_test))
print("_________________________PSVM Prediction_______________________________")
print(psvm.predict(X_test))
print("_________________________GSVM Prediction_______________________________")
print(gsvm.predict(X_test))


# 6. Appliquer cette fois les trois techniques d’ensemble learning « bagging , stacking et boosting »

# In[69]:


#bagging
from sklearn.ensemble import BaggingClassifier

models=[knn,nb,ann,clf,lsvm,psvm,gsvm]
bagging_classifier = BaggingClassifier(base_estimator=models, n_estimators=10, random_state=42)
bagging_classifier.fit(X_train, Y_train)
y_pred = bagging_classifier.predict(X_test)

# Calcul de l'accuracy
accuracy = accuracy_score(Y_test, y_pred)
print("Accuracy:", accuracy)


# In[86]:


#stacking
from sklearn.ensemble import StackingClassifier

models=[knn,clf,nb,ann,lsvm,psvm,gsvm ]
sclf = StackingClassifier(estimators=models ,final_estimator=LogisticRegression())

for mdl in models:
    mdl.fit(X_train, Y_train)

for clf in models:
    print("Accuracy: %0.2f" % mdl.score(X_test, Y_test))


# In[88]:


#boosting
from sklearn.ensemble import AdaBoostClassifier

boosting = AdaBoostClassifier(base_estimator=models)
boosting.fit(X_train, Y_train)

y_pred = boosting.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)


# In[ ]:




